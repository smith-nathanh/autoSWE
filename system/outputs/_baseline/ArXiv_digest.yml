messages:
  - UML class diagram approved
  - Code approved
  - Acceptance tests passed
  - Unit tests passed
approvals:
  UML_class: true
  UML_sequence: true
  architecture_design: true
  requirements: true
  implementation: true
  acceptance_tests: true
  unit_tests: true
documents:
  PRD: |-
    # Introduction
    Query ArXiv is a tool designed to streamline the process of fetching research papers from the ArXiv database. It allows users to perform advanced searches based on parameters like category, author, title, and abstract, with an added feature to filter results based on recent publication dates.

    # Goals
    The main goal is to create an efficient, user-friendly tool for querying the ArXiv database, enhancing the research process by offering flexible and time-sensitive search capabilities. It should also allow user to either print query results to console or save them to specified csv file.

    # Features and Functionalities
    - Advanced Query Options:
        - Enables querying by any combinations of `category`, `author`, `title`, and `abstract`. But at least one of them should be specified
        - `max_results` parameter to control the number of results, with a sensible default (recommanded: 10).
    - Time-based Filtering:
        - Integrates a mandatory `recent_days` parameter, not directly supported by ArXiv. This feature requires custom implementation:
            - **Query URL Construction:** Queries are structured with sortBy=submittedDate and sortOrder=descending to fetch recent papers first.
            - **Example Query URL:** 
            ```
            http://export.arxiv.org/api/query?search_query=cat:cs.CL+AND+au:Smith+AND+ti:neural+AND+abs:learning&sortBy=submittedDate&sortOrder=descending&start=0&max_results=10
            ```
            - **Custom Date Check:** The `check_date` function is written to filter the results based on the recent_days parameter, ensuring only papers from the specified recent period are included.

    - Output Handling:
        - Console Output for immediate viewing, controlled by --verbose.
        - CSV Export option controlled by --to_file.
        - If both specified, print to console and save to csv; else if only --to_file, only save to csv; otherwise (either only --verbose or neither specified), print to console

    - User Input Processing:
        - Command-line arguments for search parameters and output preferences.
    - Data Retrieval and Processing:
        - Efficient API interactions and XML data parsing according to user criteria.
    - Result Filtering and Formatting:
        - Applies date filtering via check_date.
        - Coherent presentation of key details in both console and CSV.
            - Both console output and CSV columns should include at least the following inforamtion:
                - `category`
                - `title`
                - `author`
                - `abstract`
                - `published`: publication date
                - `link`


    # Technical Constraints
    - The tool will be developed in Python, utilizing necessary libraries for API interaction, XML data parsing, and command-line argument parsing.
    - Compliance with ArXiv API usage guidelines and rate limits is required.
    - Accurate and reliable date handling for time-based filtering.

    # Requirements
    ## Dependencies
    - Python 3.x
    - Libraries: os, datetime, urllib, xml.etree.ElementTree, csv, and argparse
    - ArXiv API: https://info.arxiv.org/help/api/user-manual.html

    ## Usage
    To execute a query, run the following script:

    ```bash
    python query_arxiv.py 
    --category [category] 
    --title [title] 
    --author [author] 
    --abstract [abstract]
    --recent_days [number_of_days]
    [--to_file path_to_csv_file]
    [--verbose]
    ```

    At least one of the query parameters `[category, title, author, abstract]` must be provided, along with the mandatory `--recent_days` parameter. All arguments should be constructed with only characters from `"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+:."`; otherwise, a `ValueError` will be raised.

    ## Command Line Arguments (Script input)
    - category (str, optional): Category of the paper.
    - title (str, optional): Keyword for the title.
    - author (str, optional): Keyword for the author.
    - abstract (str, optional): Keyword in the abstract.
    - recent_days (int, required): Filter papers from the most recent k days.
    - to_file (str, optional): Path to save the results in CSV format.
    - verbose (Boolean, optional): Flag to print results to the console.

    # Acceptance Criteria
    - Successful execution of queries with various combinations of parameters.
    - Accurate filtering based on the recent_days parameter.
    - Correct formatting and data integrity in both console output and CSV file.
    - Compliance with performance and reliability standards, including efficient handling of API responses.

    # Terms/Concepts Explanation
    - ArXiv: An open-access archive and distribution service for scholarly articles in various scientific fields.
    - API: A set of protocols for building and interacting with software applications.
  UML_class: |-
    classDiagram
        class QueryArXiv {
            - String category
            - String title
            - String author
            - String abstract
            - int recent_days
            - int max_results
            - boolean verbose
            - String to_file
            + void executeQuery()
            + void fetchResults()
            + void filterResultsByDate()
            + void printToConsole()
            + void saveToCSV()
        }

        class ArXivAPI {
            + String constructQueryURL(String category, String title, String author, String abstract, int max_results)
            + List fetchResults(String queryURL)
        }

        class DateHandler {
            + boolean checkDate(String publishedDate, int recent_days)
        }

        class OutputHandler {
            + void printToConsole(List results)
            + void saveToCSV(List results, String filePath)
        }

        QueryArXiv --> ArXivAPI : uses
        QueryArXiv --> DateHandler : uses
        QueryArXiv --> OutputHandler : uses
  UML_sequence: |-
    sequenceDiagram
        participant User
        participant CommandLineInterface
        participant QueryArXiv
        participant ArXivAPI
        participant DateHandler
        participant OutputHandler

        User->>CommandLineInterface: Run script with arguments
        CommandLineInterface->>QueryArXiv: Parse arguments
        QueryArXiv->>ArXivAPI: Construct query URL
        ArXivAPI->>QueryArXiv: Return query URL
        QueryArXiv->>ArXivAPI: Fetch results
        ArXivAPI->>QueryArXiv: Return results
        QueryArXiv->>DateHandler: Filter results by date
        DateHandler->>QueryArXiv: Return filtered results
        alt verbose is true
            QueryArXiv->>OutputHandler: Print to console
        end
        alt to_file is specified
            QueryArXiv->>OutputHandler: Save to CSV
        end
  architecture_design: |-
    query_arxiv_project/
    │
    ├── query_arxiv.py
    ├── arxiv_api.py
    ├── date_handler.py
    ├── output_handler.py
    └── requirements.txt
  requirements: |-
    os
    urllib
    xml.etree.ElementTree
    csv
    argparse
    datetime
    unittest
  code:
    query_arxiv.py: |-
      import argparse
      from arxiv_api import ArXivAPI
      from date_handler import DateHandler
      from output_handler import OutputHandler

      class QueryArXiv:
          def __init__(self, category, title, author, abstract, recent_days, max_results=10, verbose=False, to_file=None):
              self.category = category
              self.title = title
              self.author = author
              self.abstract = abstract
              self.recent_days = recent_days
              self.max_results = max_results
              self.verbose = verbose
              self.to_file = to_file

          def execute_query(self):
              api = ArXivAPI()
              query_url = api.construct_query_url(self.category, self.title, self.author, self.abstract, self.max_results)
              results = api.fetch_results(query_url)
              filtered_results = self.filter_results_by_date(results)
              if self.verbose:
                  self.print_to_console(filtered_results)
              if self.to_file:
                  self.save_to_csv(filtered_results)

          def filter_results_by_date(self, results):
              date_handler = DateHandler()
              return [result for result in results if date_handler.check_date(result['published'], self.recent_days)]

          def print_to_console(self, results):
              output_handler = OutputHandler()
              output_handler.print_to_console(results)

          def save_to_csv(self, results):
              output_handler = OutputHandler()
              output_handler.save_to_csv(results, self.to_file)


      def main():
          parser = argparse.ArgumentParser(description='Query ArXiv for research papers.')
          parser.add_argument('--category', type=str, help='Category of the paper.')
          parser.add_argument('--title', type=str, help='Keyword for the title.')
          parser.add_argument('--author', type=str, help='Keyword for the author.')
          parser.add_argument('--abstract', type=str, help='Keyword in the abstract.')
          parser.add_argument('--recent_days', type=int, required=True, help='Filter papers from the most recent k days.')
          parser.add_argument('--to_file', type=str, help='Path to save the results in CSV format.')
          parser.add_argument('--verbose', action='store_true', help='Flag to print results to the console.')

          args = parser.parse_args()

          if not (args.category or args.title or args.author or args.abstract):
              raise ValueError('At least one of the query parameters [category, title, author, abstract] must be provided.')

          query_arxiv = QueryArXiv(
              category=args.category,
              title=args.title,
              author=args.author,
              abstract=args.abstract,
              recent_days=args.recent_days,
              verbose=args.verbose,
              to_file=args.to_file
          )
          query_arxiv.execute_query()

      if __name__ == '__main__':
          main()
    arxiv_api.py: |
      import urllib.parse
      import urllib.request
      import xml.etree.ElementTree as ET

      class ArXivAPI:
          BASE_URL = "http://export.arxiv.org/api/query?"

          def construct_query_url(self, category, title, author, abstract, max_results):
              query_parts = []
              if category:
                  query_parts.append(f"cat:{category}")
              if title:
                  query_parts.append(f"ti:{title}")
              if author:
                  query_parts.append(f"au:{author}")
              if abstract:
                  query_parts.append(f"abs:{abstract}")

              query = "+AND+".join(query_parts)
              url = f"{self.BASE_URL}search_query={query}&sortBy=submittedDate&sortOrder=descending&start=0&max_results={max_results}"
              return url

          def fetch_results(self, query_url):
              response = urllib.request.urlopen(query_url)
              xml_data = response.read()
              root = ET.fromstring(xml_data)
              results = []
              for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
                  result = {
                      'category': entry.find("{http://arxiv.org/schemas/atom}primary_category").attrib['term'],
                      'title': entry.find("{http://www.w3.org/2005/Atom}title").text,
                      'author': ", ".join(author.find("{http://www.w3.org/2005/Atom}name").text for author in entry.findall("{http://www.w3.org/2005/Atom}author")),
                      'abstract': entry.find("{http://www.w3.org/2005/Atom}summary").text.strip(),
                      'published': entry.find("{http://www.w3.org/2005/Atom}published").text,
                      'link': entry.find("{http://www.w3.org/2005/Atom}id").text
                  }
                  results.append(result)
              return results
    date_handler.py: |
      from datetime import datetime, timedelta

      class DateHandler:
          def check_date(self, published_date, recent_days):
              published_date = datetime.strptime(published_date, "%Y-%m-%dT%H:%M:%SZ")
              cutoff_date = datetime.now() - timedelta(days=recent_days)
              return published_date >= cutoff_date
    output_handler.py: |
      import csv

      class OutputHandler:
          def print_to_console(self, results):
              for result in results:
                  print(f"Category: {result['category']}")
                  print(f"Title: {result['title']}")
                  print(f"Author(s): {result['author']}")
                  print(f"Abstract: {result['abstract']}")
                  print(f"Published: {result['published']}")
                  print(f"Link: {result['link']}")
                  print("-" * 40)

          def save_to_csv(self, results, file_path):
              with open(file_path, mode='w', newline='', encoding='utf-8') as file:
                  writer = csv.DictWriter(file, fieldnames=['category', 'title', 'author', 'abstract', 'published', 'link'])
                  writer.writeheader()
                  for result in results:
                      writer.writerow(result)
    requirements.txt: |-
      os
      urllib
      xml.etree.ElementTree
      csv
      argparse
      datetime
      unittest
  acceptance_tests: |
    # Acceptance Tests for Query ArXiv Tool

    import unittest
    from query_arxiv import QueryArXiv
    from output_handler import OutputHandler
    import os

    class TestQueryArXiv(unittest.TestCase):
        
        def setUp(self):
            # Setup any state specific to the test case.
            self.query_arxiv = QueryArXiv()

        def test_query_with_all_parameters(self):
            # Test querying with all parameters
            self.query_arxiv.category = "cs.CL"
            self.query_arxiv.title = "neural"
            self.query_arxiv.author = "Smith"
            self.query_arxiv.abstract = "learning"
            self.query_arxiv.recent_days = 30
            self.query_arxiv.max_results = 10
            self.query_arxiv.verbose = True
            self.query_arxiv.to_file = "test_output.csv"

            self.query_arxiv.executeQuery()

            # Check if results are printed to console
            # Check if results are saved to CSV
            self.assertTrue(os.path.exists("test_output.csv"))

        def test_query_with_minimum_parameters(self):
            # Test querying with minimum parameters
            self.query_arxiv.category = "cs.CL"
            self.query_arxiv.recent_days = 30
            self.query_arxiv.verbose = True

            self.query_arxiv.executeQuery()

            # Check if results are printed to console
            # No CSV file should be created
            self.assertFalse(os.path.exists("test_output.csv"))

        def test_invalid_characters_in_parameters(self):
            # Test invalid characters in parameters
            with self.assertRaises(ValueError):
                self.query_arxiv.category = "cs.CL!"
                self.query_arxiv.recent_days = 30
                self.query_arxiv.executeQuery()

        def test_date_filtering(self):
            # Test date filtering
            self.query_arxiv.category = "cs.CL"
            self.query_arxiv.recent_days = 7
            self.query_arxiv.max_results = 10
            self.query_arxiv.verbose = False

            results = self.query_arxiv.fetchResults()
            filtered_results = self.query_arxiv.filterResultsByDate(results)

            # Check if all results are within the recent_days limit
            for result in filtered_results:
                self.assertTrue(self.query_arxiv.date_handler.checkDate(result['published'], self.query_arxiv.recent_days))

        def tearDown(self):
            # Clean up any state that was set up for the test
            if os.path.exists("test_output.csv"):
                os.remove("test_output.csv")

    if __name__ == '__main__':
        unittest.main()
  unit_tests: |
    import unittest
    from unittest.mock import patch, MagicMock
    from query_arxiv import QueryArXiv
    from arxiv_api import ArXivAPI
    from date_handler import DateHandler
    from output_handler import OutputHandler
    import datetime

    class TestQueryArXiv(unittest.TestCase):
        def setUp(self):
            self.query_arxiv = QueryArXiv()

        @patch.object(ArXivAPI, 'constructQueryURL')
        def test_construct_query_url(self, mock_construct_query_url):
            mock_construct_query_url.return_value = 'http://export.arxiv.org/api/query?search_query=cat:cs.CL&sortBy=submittedDate&sortOrder=descending&start=0&max_results=10'
            url = self.query_arxiv.constructQueryURL('cs.CL', '', '', '', 10)
            self.assertEqual(url, 'http://export.arxiv.org/api/query?search_query=cat:cs.CL&sortBy=submittedDate&sortOrder=descending&start=0&max_results=10')

        @patch.object(ArXivAPI, 'fetchResults')
        def test_fetch_results(self, mock_fetch_results):
            mock_fetch_results.return_value = [{'title': 'Sample Paper', 'published': '2023-10-01'}]
            results = self.query_arxiv.fetchResults('http://export.arxiv.org/api/query?search_query=cat:cs.CL')
            self.assertEqual(len(results), 1)
            self.assertEqual(results[0]['title'], 'Sample Paper')

        @patch.object(DateHandler, 'checkDate')
        def test_filter_results_by_date(self, mock_check_date):
            mock_check_date.return_value = True
            results = [{'title': 'Sample Paper', 'published': '2023-10-01'}]
            filtered_results = self.query_arxiv.filterResultsByDate(results, 30)
            self.assertEqual(len(filtered_results), 1)

        @patch.object(OutputHandler, 'printToConsole')
        def test_print_to_console(self, mock_print_to_console):
            results = [{'title': 'Sample Paper', 'published': '2023-10-01'}]
            self.query_arxiv.printToConsole(results)
            mock_print_to_console.assert_called_once_with(results)

        @patch.object(OutputHandler, 'saveToCSV')
        def test_save_to_csv(self, mock_save_to_csv):
            results = [{'title': 'Sample Paper', 'published': '2023-10-01'}]
            self.query_arxiv.saveToCSV(results, 'output.csv')
            mock_save_to_csv.assert_called_once_with(results, 'output.csv')

        def test_invalid_arguments(self):
            with self.assertRaises(ValueError):
                self.query_arxiv.executeQuery(category='cs.CL', title='@invalid', recent_days=10)

        def test_missing_recent_days(self):
            with self.assertRaises(ValueError):
                self.query_arxiv.executeQuery(category='cs.CL', title='Neural Networks')

    if __name__ == '__main__':
        unittest.main()